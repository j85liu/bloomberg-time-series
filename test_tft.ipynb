{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f5fb4c1",
   "metadata": {},
   "source": [
    "## Temporal Fusion Transformer Test V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab51d248",
   "metadata": {},
   "outputs": [
    {
     "ename": "_IncompleteInputError",
     "evalue": "incomplete input (2235146363.py, line 674)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 674\u001b[0;36m\u001b[0m\n\u001b[0;31m    if encoder_vsn_weights_samples and encoder_vsn_weights_samples[0].size > 0:\u001b[0m\n\u001b[0m                                                                               ^\u001b[0m\n\u001b[0;31m_IncompleteInputError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import math\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Import your TFT model\n",
    "# Adjust the import path as needed\n",
    "from models.tft_v2 import TemporalFusionTransformer\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create synthetic financial data\n",
    "def generate_financial_data(n_samples=1000, freq='D'):\n",
    "    \"\"\"Generate synthetic financial time series with trend, seasonality, and exogenous factors\"\"\"\n",
    "    date_range = pd.date_range(start='2020-01-01', periods=n_samples, freq=freq)\n",
    "    \n",
    "    # Time index\n",
    "    t = np.arange(n_samples)\n",
    "    \n",
    "    # Price components\n",
    "    trend = 50 + 0.05 * t  # Upward trend\n",
    "    seasonality = 5 * np.sin(2 * np.pi * t / 50) + 3 * np.sin(2 * np.pi * t / 250)  # Multiple seasonal patterns\n",
    "    noise = np.random.normal(scale=2, size=n_samples)\n",
    "    \n",
    "    # Price\n",
    "    price = trend + seasonality + noise\n",
    "    \n",
    "    # Generate exogenous variables\n",
    "    # 1. Market index - correlated with price but with its own pattern\n",
    "    market_index = 1000 + 0.03 * t + 30 * np.sin(2 * np.pi * t / 75) + np.random.normal(scale=10, size=n_samples)\n",
    "    market_index = market_index * (1 + 0.3 * (price - np.mean(price)) / np.std(price))\n",
    "    \n",
    "    # 2. Trading volume - higher on volatile days\n",
    "    volume = 10000 + 5000 * np.sin(2 * np.pi * t / 5) + 2000 * np.random.gamma(2, 1, size=n_samples)\n",
    "    volume = volume * (1 + 0.5 * np.abs(noise) / np.std(noise))\n",
    "    \n",
    "    # 3. Volatility index\n",
    "    volatility = 15 + 5 * np.sin(2 * np.pi * t / 120) + 3 * np.cumsum(np.random.normal(scale=0.05, size=n_samples))\n",
    "    volatility = np.abs(volatility)\n",
    "    \n",
    "    # 4. Interest rate (slowly changing)\n",
    "    interest_rate = 2 + 1 * np.sin(2 * np.pi * t / 500) + 0.5 * np.cumsum(np.random.normal(scale=0.01, size=n_samples))\n",
    "    \n",
    "    # 5. Day of week effect (categorical)\n",
    "    weekday = np.array([d.weekday() for d in date_range])\n",
    "    \n",
    "    # 6. Month effect (categorical)\n",
    "    month = np.array([d.month for d in date_range])\n",
    "    \n",
    "    # Create DataFrame\n",
    "    df = pd.DataFrame({\n",
    "        'date': date_range,\n",
    "        'price': price,\n",
    "        'market_index': market_index,\n",
    "        'volume': volume,\n",
    "        'volatility': volatility,\n",
    "        'interest_rate': interest_rate,\n",
    "        'weekday': weekday,\n",
    "        'month': month\n",
    "    })\n",
    "    \n",
    "    # Add some technical indicators\n",
    "    df['price_ma5'] = df['price'].rolling(window=5).mean().fillna(method='bfill')\n",
    "    df['price_ma20'] = df['price'].rolling(window=20).mean().fillna(method='bfill')\n",
    "    df['rsi'] = calculate_rsi(df['price'], 14).fillna(method='bfill')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Calculate RSI (Relative Strength Index)\n",
    "def calculate_rsi(series, window=14):\n",
    "    delta = series.diff()\n",
    "    gain = delta.clip(lower=0).rolling(window=window).mean()\n",
    "    loss = -delta.clip(upper=0).rolling(window=window).mean()\n",
    "    rs = gain / loss\n",
    "    return 100 - (100 / (1 + rs))\n",
    "\n",
    "# Generate the dataset\n",
    "df = generate_financial_data(n_samples=1500)\n",
    "\n",
    "# Plot the generated data\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "plt.subplot(3, 1, 1)\n",
    "plt.plot(df['date'], df['price'], label='Price')\n",
    "plt.plot(df['date'], df['price_ma20'], label='20-day MA', alpha=0.7)\n",
    "plt.legend()\n",
    "plt.title('Synthetic Asset Price')\n",
    "\n",
    "plt.subplot(3, 1, 2)\n",
    "plt.plot(df['date'], df['market_index'], label='Market Index')\n",
    "plt.plot(df['date'], df['volatility'] * 20, label='Volatility × 20', alpha=0.7)  # Scaled for visibility\n",
    "plt.legend()\n",
    "plt.title('Market Factors')\n",
    "\n",
    "plt.subplot(3, 1, 3)\n",
    "plt.plot(df['date'], df['volume'], label='Volume')\n",
    "plt.plot(df['date'], df['rsi'] * 1000, label='RSI × 1000', alpha=0.7)  # Scaled for visibility\n",
    "plt.legend()\n",
    "plt.title('Trading Indicators')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Prepare data for TFT\n",
    "# We'll use:\n",
    "# - Static variables: None for this example\n",
    "# - Past inputs: price history, technical indicators\n",
    "# - Future inputs: known calendar features (weekday, month)\n",
    "\n",
    "# Preprocess data\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Scale and prepare data for the TFT model\"\"\"\n",
    "    \n",
    "    # Create date features\n",
    "    df['day_sin'] = np.sin(2 * np.pi * df['weekday'] / 7)\n",
    "    df['day_cos'] = np.cos(2 * np.pi * df['weekday'] / 7)\n",
    "    df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "    df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "    \n",
    "    # Create scaled versions of numerical columns\n",
    "    price_scaler = StandardScaler()\n",
    "    market_scaler = StandardScaler()\n",
    "    volume_scaler = StandardScaler()\n",
    "    other_scaler = StandardScaler()\n",
    "    \n",
    "    # Price-related features\n",
    "    price_cols = ['price', 'price_ma5', 'price_ma20']\n",
    "    df[['price_scaled', 'price_ma5_scaled', 'price_ma20_scaled']] = price_scaler.fit_transform(df[price_cols])\n",
    "    \n",
    "    # Market features\n",
    "    market_cols = ['market_index', 'volatility']\n",
    "    df[['market_index_scaled', 'volatility_scaled']] = market_scaler.fit_transform(df[market_cols])\n",
    "    \n",
    "    # Volume\n",
    "    df['volume_scaled'] = volume_scaler.fit_transform(df[['volume']])\n",
    "    \n",
    "    # Other features\n",
    "    other_cols = ['interest_rate', 'rsi']\n",
    "    df[['interest_rate_scaled', 'rsi_scaled']] = other_scaler.fit_transform(df[other_cols])\n",
    "    \n",
    "    return df, {\n",
    "        'price': price_scaler,\n",
    "        'market': market_scaler,\n",
    "        'volume': volume_scaler,\n",
    "        'other': other_scaler\n",
    "    }\n",
    "\n",
    "# Preprocess the data\n",
    "df, scalers = preprocess_data(df)\n",
    "\n",
    "# Define parameters\n",
    "lookback_period = 30  # 30 days history\n",
    "forecast_horizon = 5  # Predict 5 days ahead\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(df, lookback, horizon):\n",
    "    \"\"\"Create sequences for time series forecasting\"\"\"\n",
    "    X_past_price = []  # Past price & indicators\n",
    "    X_past_market = []  # Past market data\n",
    "    X_future_calendar = []  # Future calendar features\n",
    "    \n",
    "    y = []  # Target prices\n",
    "    \n",
    "    for i in range(len(df) - lookback - horizon + 1):\n",
    "        # Past data\n",
    "        past_data = df.iloc[i:i+lookback]\n",
    "        \n",
    "        # Past price features\n",
    "        past_price_features = past_data[['price_scaled', 'price_ma5_scaled', 'price_ma20_scaled', 'rsi_scaled']].values\n",
    "        X_past_price.append(past_price_features)\n",
    "        \n",
    "        # Past market features\n",
    "        past_market_features = past_data[['market_index_scaled', 'volatility_scaled', 'volume_scaled', 'interest_rate_scaled']].values\n",
    "        X_past_market.append(past_market_features)\n",
    "        \n",
    "        # Future data - known calendar features\n",
    "        future_data = df.iloc[i+lookback:i+lookback+horizon]\n",
    "        future_calendar_features = future_data[['day_sin', 'day_cos', 'month_sin', 'month_cos']].values\n",
    "        X_future_calendar.append(future_calendar_features)\n",
    "        \n",
    "        # Target - future prices\n",
    "        target_prices = future_data['price_scaled'].values\n",
    "        y.append(target_prices)\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(np.array(X_past_price), dtype=torch.float32),\n",
    "        torch.tensor(np.array(X_past_market), dtype=torch.float32),\n",
    "        torch.tensor(np.array(X_future_calendar), dtype=torch.float32),\n",
    "        torch.tensor(np.array(y), dtype=torch.float32)\n",
    "    )\n",
    "\n",
    "# Create sequences\n",
    "X_past_price, X_past_market, X_future_calendar, y = create_sequences(df, lookback_period, forecast_horizon)\n",
    "\n",
    "# Train/test split (80/20)\n",
    "train_size = int(0.8 * len(X_past_price))\n",
    "test_size = len(X_past_price) - train_size\n",
    "\n",
    "X_past_price_train, X_past_price_test = X_past_price[:train_size], X_past_price[train_size:]\n",
    "X_past_market_train, X_past_market_test = X_past_market[:train_size], X_past_market[train_size:]\n",
    "X_future_calendar_train, X_future_calendar_test = X_future_calendar[:train_size], X_future_calendar[train_size:]\n",
    "y_train, y_test = y[:train_size], y[train_size:]\n",
    "\n",
    "print(f\"Training set size: {train_size} sequences\")\n",
    "print(f\"Test set size: {test_size} sequences\")\n",
    "print(f\"Each sequence has {lookback_period} historical timesteps and predicts {forecast_horizon} future timesteps\")\n",
    "\n",
    "# Setup TFT model\n",
    "encoder_input_sizes = [4, 4]  # Past price features, past market features\n",
    "decoder_input_sizes = [4]     # Future calendar features\n",
    "hidden_dim = 64\n",
    "num_heads = 4\n",
    "dropout = 0.1\n",
    "num_lstm_layers = 2\n",
    "\n",
    "model = TemporalFusionTransformer(\n",
    "    num_static_vars=0,\n",
    "    num_future_vars=1,\n",
    "    num_past_vars=2,\n",
    "    static_input_sizes=[],\n",
    "    encoder_input_sizes=encoder_input_sizes,\n",
    "    decoder_input_sizes=decoder_input_sizes,\n",
    "    hidden_dim=hidden_dim,\n",
    "    lstm_layers=num_lstm_layers,\n",
    "    lstm_dropout=dropout,\n",
    "    dropout=dropout,\n",
    "    num_heads=num_heads,\n",
    "    forecast_horizon=forecast_horizon,\n",
    "    backcast_length=lookback_period,\n",
    "    output_dim=1,\n",
    "    quantiles=[0.1, 0.5, 0.9]\n",
    ").to(device)\n",
    "\n",
    "# Training function\n",
    "def train_model(model, epochs=50, batch_size=32, patience=10):\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.5, patience=5, verbose=True)\n",
    "    \n",
    "    # Quantile loss function\n",
    "    def quantile_loss(y_pred, y_true, quantiles):\n",
    "        losses = []\n",
    "        for i, q in enumerate([0.1, 0.5, 0.9]):\n",
    "            errors = y_true - y_pred[:, :, :, i]\n",
    "            losses.append(torch.max((q - 1) * errors, q * errors).mean())\n",
    "        return torch.mean(torch.stack(losses))\n",
    "    \n",
    "    # For tracking\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    \n",
    "    # Progress trackers for plotting\n",
    "    epoch_progress = []\n",
    "    batch_progress = []\n",
    "    batch_loss_values = []\n",
    "    \n",
    "    # Create mini-batches\n",
    "    def get_batches(x_past_price, x_past_market, x_future_calendar, y_target, batch_size):\n",
    "        for i in range(0, len(x_past_price), batch_size):\n",
    "            yield (\n",
    "                x_past_price[i:i+batch_size].to(device),\n",
    "                x_past_market[i:i+batch_size].to(device),\n",
    "                x_future_calendar[i:i+batch_size].to(device),\n",
    "                y_target[i:i+batch_size].to(device)\n",
    "            )\n",
    "    \n",
    "    model.train()\n",
    "    total_batches = len(X_past_price_train) // batch_size + (1 if len(X_past_price_train) % batch_size != 0 else 0)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        batch_count = 0\n",
    "        \n",
    "        # Shuffle training data\n",
    "        indices = torch.randperm(len(X_past_price_train))\n",
    "        X_past_price_train_shuffled = X_past_price_train[indices]\n",
    "        X_past_market_train_shuffled = X_past_market_train[indices]\n",
    "        X_future_calendar_train_shuffled = X_future_calendar_train[indices]\n",
    "        y_train_shuffled = y_train[indices]\n",
    "        \n",
    "        # Training loop\n",
    "        for batch_idx, (x_past_price_batch, x_past_market_batch, x_future_calendar_batch, y_batch) in enumerate(\n",
    "            get_batches(X_past_price_train_shuffled, X_past_market_train_shuffled, \n",
    "                       X_future_calendar_train_shuffled, y_train_shuffled, batch_size)\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Prepare encoder/decoder inputs as lists\n",
    "            encoder_inputs = [x_past_price_batch, x_past_market_batch]\n",
    "            decoder_inputs = [x_future_calendar_batch]\n",
    "            \n",
    "            # Forward pass\n",
    "            y_batch = y_batch.unsqueeze(-1)  # Add feature dimension to match model output\n",
    "            predictions = model(static_inputs=None, encoder_inputs=encoder_inputs, decoder_inputs=decoder_inputs)\n",
    "            \n",
    "            # Calculate loss\n",
    "            loss = quantile_loss(predictions, y_batch, [0.1, 0.5, 0.9])\n",
    "            \n",
    "            # Backward and optimize\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Track progress\n",
    "            epoch_loss += loss.item()\n",
    "            batch_count += 1\n",
    "            \n",
    "            # For plotting\n",
    "            epoch_progress.append(epoch + batch_idx/total_batches)\n",
    "            batch_progress.append(epoch * total_batches + batch_idx)\n",
    "            batch_loss_values.append(loss.item())\n",
    "            \n",
    "            # Print batch progress every few batches\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{total_batches}, Loss: {loss.item():.4f}\")\n",
    "        \n",
    "        # Calculate validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_batch_count = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_past_price_batch, x_past_market_batch, x_future_calendar_batch, y_batch in get_batches(\n",
    "                X_past_price_test, X_past_market_test, X_future_calendar_test, y_test, batch_size\n",
    "            ):\n",
    "                # Prepare encoder/decoder inputs as lists\n",
    "                encoder_inputs = [x_past_price_batch, x_past_market_batch]\n",
    "                decoder_inputs = [x_future_calendar_batch]\n",
    "                \n",
    "                # Forward pass\n",
    "                y_batch = y_batch.unsqueeze(-1)\n",
    "                predictions = model(static_inputs=None, encoder_inputs=encoder_inputs, decoder_inputs=decoder_inputs)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = quantile_loss(predictions, y_batch, [0.1, 0.5, 0.9])\n",
    "                val_loss += loss.item()\n",
    "                val_batch_count += 1\n",
    "        \n",
    "        avg_train_loss = epoch_loss / batch_count\n",
    "        avg_val_loss = val_loss / val_batch_count\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_val_loss)\n",
    "        \n",
    "        # Track best model\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch\n",
    "            patience_counter = 0\n",
    "            # Save best model\n",
    "            torch.save(model.state_dict(), 'best_tft_model.pth')\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping triggered after epoch {epoch+1}\")\n",
    "            break\n",
    "        \n",
    "        # Switch back to training mode\n",
    "        model.train()\n",
    "    \n",
    "    # Load best model\n",
    "    model.load_state_dict(torch.load('best_tft_model.pth'))\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'best_epoch': best_epoch,\n",
    "        'epoch_progress': epoch_progress,\n",
    "        'batch_progress': batch_progress,\n",
    "        'batch_loss_values': batch_loss_values\n",
    "    }\n",
    "\n",
    "# Train the model\n",
    "history = train_model(model, epochs=50, batch_size=32, patience=10)\n",
    "\n",
    "# Plot training curves\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Training loss curve\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(history['train_losses'], label='Training Loss')\n",
    "plt.plot(history['val_losses'], label='Validation Loss')\n",
    "plt.axvline(x=history['best_epoch'], color='r', linestyle='--', label=f'Best Model (Epoch {history[\"best_epoch\"]+1})')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('TFT Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Batch loss curve\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(history['batch_progress'], history['batch_loss_values'], alpha=0.3, label='Batch Loss')\n",
    "# Add a smoothed version\n",
    "window_size = 20\n",
    "if len(history['batch_loss_values']) > window_size:\n",
    "    smoothed = np.convolve(history['batch_loss_values'], np.ones(window_size)/window_size, mode='valid')\n",
    "    plt.plot(history['batch_progress'][window_size-1:], smoothed, label=f'Moving Avg (window={window_size})')\n",
    "plt.xlabel('Batch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('TFT Training Loss by Batch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate the model\n",
    "def evaluate_model(model, X_past_price, X_past_market, X_future_calendar, y_true, scalers):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    actual = []\n",
    "    \n",
    "    batch_size = 64\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_past_price), batch_size):\n",
    "            # Prepare batches\n",
    "            x_past_price_batch = X_past_price[i:i+batch_size].to(device)\n",
    "            x_past_market_batch = X_past_market[i:i+batch_size].to(device)\n",
    "            x_future_calendar_batch = X_future_calendar[i:i+batch_size].to(device)\n",
    "            y_batch = y_true[i:i+batch_size]\n",
    "            \n",
    "            # Prepare inputs as lists\n",
    "            encoder_inputs = [x_past_price_batch, x_past_market_batch]\n",
    "            decoder_inputs = [x_future_calendar_batch]\n",
    "            \n",
    "            # Forward pass - get all quantiles\n",
    "            output = model(static_inputs=None, encoder_inputs=encoder_inputs, decoder_inputs=decoder_inputs)\n",
    "            \n",
    "            # Extract median prediction (quantile 0.5 is index 1)\n",
    "            median_preds = output[:, :, 0, 1].cpu()  # [batch, horizon, feature=0, quantile=0.5]\n",
    "            \n",
    "            # Store predictions and actual values\n",
    "            predictions.append(median_preds)\n",
    "            actual.append(y_batch)\n",
    "    \n",
    "    # Concatenate all batches\n",
    "    predictions = torch.cat(predictions, dim=0).numpy()\n",
    "    actual = torch.cat(actual, dim=0).numpy()\n",
    "    \n",
    "    # Inverse transform to get original scale\n",
    "    predictions_orig = scalers['price'].inverse_transform(predictions.reshape(-1, 1)).reshape(predictions.shape)\n",
    "    actual_orig = scalers['price'].inverse_transform(actual.reshape(-1, 1)).reshape(actual.shape)\n",
    "    \n",
    "    return predictions, actual, predictions_orig, actual_orig\n",
    "\n",
    "# Evaluate on train and test sets\n",
    "train_preds, train_actual, train_preds_orig, train_actual_orig = evaluate_model(\n",
    "    model, X_past_price_train, X_past_market_train, X_future_calendar_train, y_train, scalers\n",
    ")\n",
    "\n",
    "test_preds, test_actual, test_preds_orig, test_actual_orig = evaluate_model(\n",
    "    model, X_past_price_test, X_past_market_test, X_future_calendar_test, y_test, scalers\n",
    ")\n",
    "\n",
    "# Calculate metrics\n",
    "def calculate_metrics(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true.flatten(), y_pred.flatten())\n",
    "    rmse = math.sqrt(mse)\n",
    "    mae = mean_absolute_error(y_true.flatten(), y_pred.flatten())\n",
    "    r2 = r2_score(y_true.flatten(), y_pred.flatten())\n",
    "    \n",
    "    # Mean absolute percentage error\n",
    "    mape = np.mean(np.abs((y_true.flatten() - y_pred.flatten()) / y_true.flatten())) * 100\n",
    "    \n",
    "    # Directional accuracy (for financial data)\n",
    "    y_true_diff = np.diff(y_true, axis=1)\n",
    "    y_pred_diff = np.diff(y_pred, axis=1)\n",
    "    directional_acc = np.mean((y_true_diff * y_pred_diff) > 0) * 100\n",
    "    \n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae,\n",
    "        'R²': r2,\n",
    "        'MAPE': mape,\n",
    "        'Direction Accuracy': directional_acc\n",
    "    }\n",
    "\n",
    "# Calculate metrics for train and test sets\n",
    "train_metrics = calculate_metrics(train_actual_orig, train_preds_orig)\n",
    "test_metrics = calculate_metrics(test_actual_orig, test_preds_orig)\n",
    "\n",
    "# Print metrics\n",
    "print(\"\\nTraining Set Metrics:\")\n",
    "for metric, value in train_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for metric, value in test_metrics.items():\n",
    "    print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "# Plot forecasts\n",
    "def plot_predictions(actual, predictions, split_idx, forecast_horizon=5, num_samples=100):\n",
    "    \"\"\"Plot predictions vs actual values\"\"\"\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Select a subset of data points for clarity\n",
    "    indices = np.linspace(0, len(actual) - 1, num_samples, dtype=int)\n",
    "    \n",
    "    # Create x-axis with appropriate time indexing\n",
    "    x = np.array(list(range(len(indices) * forecast_horizon)))\n",
    "    \n",
    "    # Initialize arrays for actual and predicted values\n",
    "    y_true_plot = np.zeros(len(indices) * forecast_horizon)\n",
    "    y_pred_plot = np.zeros(len(indices) * forecast_horizon)\n",
    "    \n",
    "    # Populate arrays\n",
    "    for i, idx in enumerate(indices):\n",
    "        for h in range(forecast_horizon):\n",
    "            true_idx = i * forecast_horizon + h\n",
    "            if true_idx < len(y_true_plot):\n",
    "                y_true_plot[true_idx] = actual[idx, h]\n",
    "                y_pred_plot[true_idx] = predictions[idx, h]\n",
    "    \n",
    "    # Split index for plot (convert from sequence index to plot index)\n",
    "    plot_split_idx = (split_idx / len(actual)) * len(y_true_plot)\n",
    "    \n",
    "    # Plot\n",
    "    plt.plot(x, y_true_plot, 'b-', label='True Price', alpha=0.7)\n",
    "    plt.plot(x, y_pred_plot, 'r--', label='Predicted Price', alpha=0.7)\n",
    "    \n",
    "    # Draw vertical line for train/test split\n",
    "    plt.axvline(x=plot_split_idx, color='green', linestyle='--', label='Train/Test Split')\n",
    "    \n",
    "    plt.title('Asset Price - True vs Predicted')\n",
    "    plt.xlabel('Time Steps')\n",
    "    plt.ylabel('Price')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Combine train and test data for a continuous plot\n",
    "all_actual = np.vstack([train_actual_orig, test_actual_orig])\n",
    "all_preds = np.vstack([train_preds_orig, test_preds_orig])\n",
    "\n",
    "# Plot predictions\n",
    "plot_predictions(all_actual, all_preds, train_size, forecast_horizon=forecast_horizon)\n",
    "\n",
    "# Visualize attention weights\n",
    "def visualize_attention_weights(model, sample_idx=0):\n",
    "    \"\"\"Visualize attention weights for a sample input\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Prepare inputs for a single sample\n",
    "        x_past_price = X_past_price_test[sample_idx:sample_idx+1].to(device)\n",
    "        x_past_market = X_past_market_test[sample_idx:sample_idx+1].to(device)\n",
    "        x_future_calendar = X_future_calendar_test[sample_idx:sample_idx+1].to(device)\n",
    "        \n",
    "        # Prepare inputs as lists\n",
    "        encoder_inputs = [x_past_price, x_past_market]\n",
    "        decoder_inputs = [x_future_calendar]\n",
    "        \n",
    "        # Extract attention weights (hook the attention layer)\n",
    "        attention_weights = None\n",
    "        \n",
    "        def hook_fn(module, input, output):\n",
    "            nonlocal attention_weights\n",
    "            # output is (output, attention_weights)\n",
    "            attention_weights = output[1]\n",
    "        \n",
    "        # Register a forward hook on the attention layer\n",
    "        hook = model.attention.register_forward_hook(hook_fn)\n",
    "        \n",
    "        # Forward pass\n",
    "        _ = model(static_inputs=None, encoder_inputs=encoder_inputs, decoder_inputs=decoder_inputs)\n",
    "        \n",
    "        # Remove the hook\n",
    "        hook.remove()\n",
    "        \n",
    "        # Process attention weights\n",
    "        if attention_weights is not None:\n",
    "            # Average across attention heads\n",
    "            avg_attn = attention_weights.mean(dim=1).squeeze(0).cpu().numpy()\n",
    "            \n",
    "            # Plot attention heatmap\n",
    "            plt.figure(figsize=(10, 8))\n",
    "            sns.heatmap(\n",
    "                avg_attn, \n",
    "                cmap='viridis',\n",
    "                xticklabels=[f\"t-{lookback_period-i}\" for i in range(lookback_period)] + [f\"t+{i+1}\" for i in range(forecast_horizon)],\n",
    "                yticklabels=[f\"t-{lookback_period-i}\" for i in range(lookback_period)] + [f\"t+{i+1}\" for i in range(forecast_horizon)]\n",
    "            )\n",
    "            plt.title('Self-Attention Weights Heatmap')\n",
    "            plt.xlabel('Key Positions')\n",
    "            plt.ylabel('Query Positions')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "# Try to visualize attention weights if the model has the attention layer exposed\n",
    "try:\n",
    "    visualize_attention_weights(model)\n",
    "except Exception as e:\n",
    "    print(f\"Could not visualize attention weights: {e}\")\n",
    "    print(\"Continuing with other visualizations...\")\n",
    "\n",
    "# # Visualize variable selection weights\n",
    "# def visualize_variable_selection(model, num_samples=5):\n",
    "#     \"\"\"Visualize variable selection weights for a few samples\"\"\"\n",
    "#     model.eval()\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         # Prepare inputs for a few samples\n",
    "#         sample_indices = np.random.choice(len(X_past_price_test), size=num_samples, replace=False)\n",
    "        \n",
    "#         encoder_vsn_weights_samples = []\n",
    "#         decoder_vsn_weights_samples = []\n",
    "        \n",
    "#         for idx in sample_indices:\n",
    "#             # Prepare inputs\n",
    "#             x_past_price = X_past_price_test[idx:idx+1].to(device)\n",
    "#             x_past_market = X_past_market_test[idx:idx+1].to(device)\n",
    "#             x_future_calendar = X_future_calendar_test[idx:idx+1].to(device)\n",
    "            \n",
    "#             # Prepare inputs as lists\n",
    "#             encoder_inputs = [x_past_price, x_past_market]\n",
    "#             decoder_inputs = [x_future_calendar]\n",
    "            \n",
    "#             # Extract VSN weights (we need to modify the model or use hooks)\n",
    "#             encoder_vsn_weights = None\n",
    "#             decoder_vsn_weights = None\n",
    "            \n",
    "#             def encoder_hook_fn(module, input, output):\n",
    "#                 nonlocal encoder_vsn_weights\n",
    "#                 # output is typically (combined, weights)\n",
    "#                 encoder_vsn_weights = output[1]\n",
    "            \n",
    "#             def decoder_hook_fn(module, input, output):\n",
    "#                 nonlocal decoder_vsn_weights\n",
    "#                 decoder_vsn_weights = output[1]\n",
    "            \n",
    "#             # Register hooks on VSN modules\n",
    "#             encoder_hook = model.encoder_vsn.register_forward_hook(encoder_hook_fn)\n",
    "#             decoder_hook = model.decoder_vsn.register_forward_hook(decoder_hook_fn)\n",
    "            \n",
    "#             # Forward pass\n",
    "#             _ = model(static_inputs=None, encoder_inputs=encoder_inputs, decoder_inputs=decoder_inputs)\n",
    "            \n",
    "#             # Remove hooks\n",
    "#             encoder_hook.remove()\n",
    "#             decoder_hook.remove()\n",
    "            \n",
    "#             # Collect weights\n",
    "#             if encoder_vsn_weights is not None:\n",
    "#                 encoder_vsn_weights_samples.append(encoder_vsn_weights.squeeze().cpu().numpy())\n",
    "            \n",
    "#             if decoder_vsn_weights is not None:\n",
    "#                 decoder_vsn_weights_samples.append(decoder_vsn_weights.squeeze().cpu().numpy())\n",
    "        \n",
    "#         # Plot variable selection weights\n",
    "#         if encoder_vsn_weights_samples and encoder_vsn_weights_samples[0].size > 0:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8ce150",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "finance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
